{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6456f8ac",
   "metadata": {},
   "source": [
    "# Serving Gradio Apps on Union.ai\n",
    "\n",
    "This document shows how to serve Gradio apps on Union.ai for several use cases.\n",
    "- [Simple Gradio App with text input](#simple-gradio-app)\n",
    "- [Gradio Chat interface for LLMs](#gradio-app-with-a-model)\n",
    "- [Upload or take a photo for computer vision](#gradio-app-with-a-model-and-a-dataset)\n",
    "\n",
    "Gradio is a Python library that allows you to quickly create user interfaces for machine learning models. It provides a simple way to create web-based applications that can be used for model inference, data visualization, and more.\n",
    "\n",
    "Union.ai is a platform that allows you to deploy and manage machine learning models and applications. It provides a simple way to serve Gradio apps, making it easy to share your work with others.\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "#### Install packages & clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/gradio-serve-union\n",
    "    %cd https://github.com/unionai-oss/gradio-serve-union\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c0428",
   "metadata": {},
   "source": [
    "#### Authenticate to Union.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Authenticate to union serverless\n",
    "!union create login --serverless --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db0c9e",
   "metadata": {},
   "source": [
    "Once you get your onboarding email you should be ready to the the following code cells to serve the Gradio applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1c63e",
   "metadata": {},
   "source": [
    "## Simple Gradio App with text input, sliders, and output\n",
    "\n",
    "This simple demo shows how to create a Gradio app that takes text input and outputs the same text. It also includes a slider for adjusting the output length. It can serve as a starting point for more complex applications. \n",
    "\n",
    "See the next example for a more complex Gradio app that uses a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d83910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Run this command to deploy the application\n",
    "!union deploy apps 0_simple_app/app.py gradio-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18498fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% writefile 0_simple_app/app.py\n",
    "\n",
    "\"\"\"\n",
    "# Simple Gradio App Deployment Example\n",
    "\"\"\"\n",
    "from datetime import timedelta\n",
    "from union import Resources\n",
    "from union.app import App, ScalingMetric\n",
    "from containers import container_image\n",
    "\n",
    "gradio_app = App(\n",
    "    name=\"gradio-app\",\n",
    "    container_image=container_image, # image that contains the environment and dependencies needed to run the app\n",
    "    port=8080, # The port on which the app will be served\n",
    "    include=[\"./main.py\"],  # Include your gradio code\n",
    "    args=[\"python\", \"main.py\"], # Command to run your app inside the container\n",
    "    limits=Resources(cpu=\"2\", mem=\"8Gi\"), # Maximum resources allocated (CPU, memory, GPU) â€” hard limit\n",
    "    requests=Resources(cpu=\"2\", mem=\"8Gi\"), # Minimum resources requested from the scheduler â€” soft requirement\n",
    "    min_replicas=0, # Minimum number of instances (pods) running â€” allows scale-to-zero when idle\n",
    "    max_replicas=1, # Maximum number of instances â€” restricts auto-scaling to 1 replica\n",
    "    scaledown_after=timedelta(minutes=5), # Time to wait before scaling down when traffic is low\n",
    "    scaling_metric=ScalingMetric.Concurrency(2), # Auto-scaling based on concurrent user requests; 2 concurrent users per replica\n",
    "    # requires_auth=False # Uncomment to make app public.\n",
    ")\n",
    "\n",
    "# union deploy apps 0_simple_app/app.py gradio-app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% writefile 0_simple_app/main.py\n",
    "\n",
    "\"\"\"\n",
    "This is a simple Gradio app that takes a name and an intensity level as input,\n",
    "and returns a greeting message. The app is designed to be deployed using Union from app.py.\n",
    "\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=8080)\n",
    "\n",
    "\n",
    "# union deploy apps 0_simple_app/app.py gradio-app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475e1ab",
   "metadata": {},
   "source": [
    "## Serving a Chat interface and LLM on Union.ai\n",
    "\n",
    "\n",
    "Find the code for this example in 1_llm_chat folder. \n",
    "- model.py: This file contains a task to download the LLM from Hugging Face and store it as a Union artifact.\n",
    "- app.py This contains the environment, compute, and scaling configuration for the app.\n",
    "- main.py contains the Gradio app code for the chat interface. (This used the Qwen token filtering, you may need to change how the chat is handled if you choose a different model)\n",
    "\n",
    "Explore the code in the files or see the code below in the notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Run this task to download the model files for the LLM chat example.\n",
    "!union run --remote 1_llm_chat/model.py download_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac208b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Run this command to deploy the chatbot app with the model downloaded above\n",
    "!union deploy apps 1_llm_chat/app.py gradio-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 1_llm_chat/model.py\n",
    "\n",
    "\"\"\"\n",
    "This file downloads the Qwen-3 model and saves it to a specified directory.\n",
    "You can adjust the model to another from Hugging Face by changing the model_name parameter.\n",
    "\"\"\"\n",
    "\n",
    "from union import Resources, task, Artifact, FlyteDirectory, current_context, ImageSpec\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from pathlib import Path\n",
    "from typing import Annotated\n",
    "from containers import container_image\n",
    "\n",
    "# Create Union Artifact \n",
    "Qwen3Model8b = Artifact(name=\"qwen3-model\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Download the model\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    cache=True,\n",
    "    cache_version=\"1.0\",\n",
    "    requests=Resources(cpu=\"2\", mem=\"9Gi\"),\n",
    ")\n",
    "def download_model(\n",
    "    model_name: str = \"Qwen/Qwen3-0.6B\",\n",
    ") -> Annotated[FlyteDirectory, Qwen3Model8b]:\n",
    "\n",
    "    working_dir = Path(current_context().working_directory)\n",
    "    saved_model_dir = working_dir / \"saved_model\"\n",
    "    saved_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model.save_pretrained(saved_model_dir)\n",
    "    tokenizer.save_pretrained(saved_model_dir)\n",
    "\n",
    "    # return FlyteDirectory(saved_model_dir)\n",
    "    return Qwen3Model8b.create_from(saved_model_dir)\n",
    "\n",
    "# union run --remote 1_llm_chat/model.py download_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb91779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 1_llm_chat/app.py\n",
    "\"\"\"\n",
    "This deployment script is for a Gradio app that serves as a chat interface for the Qwen-3 model.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import timedelta\n",
    "from union import Artifact, Resources\n",
    "from union.app import App, Input, ScalingMetric\n",
    "from flytekit.extras.accelerators import L4\n",
    "from containers import container_image\n",
    "\n",
    "# Point to your object detection model artifact\n",
    "Qwen3Model8b = Artifact(name=\"qwen3-model\")\n",
    "\n",
    "# Define the Gradio app deployment\n",
    "gradio_app = App(\n",
    "    name=\"gradio-chat\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"downloaded-model\",\n",
    "            value=Qwen3Model8b.query(),\n",
    "            download=True,\n",
    "        )\n",
    "    ],\n",
    "    container_image=container_image, # image that contains the environment and dependencies needed to run the app\n",
    "    port=8080, # The port on which the app will be served\n",
    "    include=[\"./main.py\"],  # Include your gradio app code\n",
    "    args=[\"python\", \"main.py\"], # Command to run your app inside the container\n",
    "    limits=Resources(cpu=\"2\", mem=\"16Gi\", gpu=\"1\"),  # Maximum resources allocated (CPU, memory, GPU) â€” hard limit\n",
    "    requests=Resources(cpu=\"2\", mem=\"16Gi\", gpu=\"1\"), # Minimum resources requested from the scheduler â€” soft requirement\n",
    "    accelerator=L4,  # Specifies the GPU type to use (e.g., NVIDIA L4 accelerator)\n",
    "    min_replicas=0, # Minimum number of instances (pods) running â€” allows scale-to-zero when idle\n",
    "    max_replicas=1, # Maximum number of instances â€” restricts auto-scaling to 1 replica\n",
    "    scaledown_after=timedelta(minutes=5), # Time to wait before scaling down when traffic is low\n",
    "    scaling_metric=ScalingMetric.Concurrency(2), # Auto-scaling based on concurrent user requests; 2 concurrent users per replica\n",
    "    # requires_auth=False # Uncomment to make app public.\n",
    ")\n",
    "\n",
    "# union deploy apps 1_llm_chat/app.py gradio-chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 1_llm_chat/main.py\n",
    "\n",
    "\"\"\"\n",
    "This creates a Gradio app that serves as a chat interface for the Qwen-3 model.\n",
    "The thinking and answer tokems are streamed separately, allowing for a more interactive experience.\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gradio as gr\n",
    "from union_runtime import get_input\n",
    "import threading\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "# --------------------------\n",
    "# Load model path from Union artifact input\n",
    "# --------------------------\n",
    "model_path = get_input(\"downloaded-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).cuda()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"\n",
    "    Function to handle chat messages and generate responses.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True, enable_thinking=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    thread = threading.Thread(\n",
    "        target=model.generate,\n",
    "        kwargs={\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"streamer\": streamer,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.3,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        },\n",
    "    )\n",
    "    thread.start()\n",
    "\n",
    "    thinking_prefix = \"ðŸ¤” **Thinking:**\\n\"\n",
    "    answer_prefix = \"\\n\\nðŸ§  **Answer:**\\n\"\n",
    "\n",
    "    current_section = \"thinking\"\n",
    "    yielded_thinking = \"\"\n",
    "    yielded_answer = \"\"\n",
    "\n",
    "    for token in streamer:\n",
    "        if current_section == \"thinking\":\n",
    "            yielded_thinking += token\n",
    "            if \"</think>\" in yielded_thinking:\n",
    "                # Split at </think> and switch to answer phase\n",
    "                thinking_text, remainder = yielded_thinking.split(\"</think>\", 1)\n",
    "                yield thinking_prefix + thinking_text.strip()\n",
    "                current_section = \"answer\"\n",
    "                yielded_answer += remainder\n",
    "                yield answer_prefix + yielded_answer.strip()\n",
    "            else:\n",
    "                yield thinking_prefix + yielded_thinking.strip()\n",
    "        else:\n",
    "            yielded_answer += token\n",
    "            yield answer_prefix + yielded_answer.strip()\n",
    "\n",
    "# --------------------------\n",
    "# Define Gradio interface\n",
    "# --------------------------\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"Qwen3 Chatbot\",\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me anything...\", container=True, scale=7),\n",
    "    multimodal=False,\n",
    "    theme=\"default\",\n",
    "    type=\"messages\",\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Launch Gradio app\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    chat_interface.launch(server_name=\"0.0.0.0\", server_port=8080)\n",
    "\n",
    "# union deploy apps 1_llm_chat/app.py gradio-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66807591",
   "metadata": {},
   "source": [
    "## Visual Language Model (VLM) with Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeed35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Run this task to download the model files for the VLM chat example.\n",
    "!union run --remote 2_cv_images/model.py download_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Run this command to deploy the Gradio app with the model downloaded above\n",
    "!union deploy apps 2_cv_images/app.py vlm-gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef898a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile 2_cv_images/model.py\n",
    "\"\"\"\n",
    "This task downloads the SmolVLM-Instruct model from Hugging Face and saves it to a specified directory.\n",
    "You can adjust the model to another from Hugging Face VLM by changing the model_name parameter.\n",
    "\"\"\"\n",
    "\n",
    "from union import Resources, task, Artifact, FlyteDirectory, current_context, ImageSpec\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from pathlib import Path\n",
    "from typing import Annotated\n",
    "from containers import container_image\n",
    "\n",
    "# Create Union Artifact \n",
    "SmolVLM = Artifact(name=\"SmolVLM-Instruct\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Download the model\n",
    "# ----------------------------------------------------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    cache=True,\n",
    "    cache_version=\"1.0\",\n",
    "    requests=Resources(cpu=\"2\", mem=\"9Gi\"),\n",
    ")\n",
    "def download_model(\n",
    "    model_name: str = \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    ") -> Annotated[FlyteDirectory, SmolVLM]:\n",
    "\n",
    "    working_dir = Path(current_context().working_directory)\n",
    "    saved_model_dir = working_dir / \"saved_model\"\n",
    "    saved_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    model.save_pretrained(saved_model_dir)\n",
    "    processor.save_pretrained(saved_model_dir)\n",
    "\n",
    "    return SmolVLM.create_from(saved_model_dir)\n",
    "\n",
    "\n",
    "# union run --remote 2_cv_images/model.py download_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 2_cv_images/app.py\n",
    "\"\"\"\n",
    "This serves as a deployment script for the SmolVLM-Instruct model with a Gradio app.\n",
    "\"\"\"\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from union import Artifact, Resources\n",
    "from union.app import App, Input, ScalingMetric\n",
    "from flytekit.extras.accelerators import L4\n",
    "from containers import container_image\n",
    "\n",
    "# Point to VLM artifact\n",
    "\n",
    "SmolVLM = Artifact(name=\"SmolVLM-Instruct\")\n",
    "\n",
    "gradio_app = App(\n",
    "    name=\"vlm-gradio\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"downloaded-model\",\n",
    "            value=SmolVLM.query(),\n",
    "            download=True,\n",
    "        )\n",
    "    ],\n",
    "    container_image=container_image,\n",
    "    port=8080,\n",
    "    include=[\"./main.py\"],  # Include your gradio app\n",
    "    args=[\"python\", \"main.py\"],\n",
    "    limits=Resources(cpu=\"2\", mem=\"24Gi\", gpu=\"1\", ephemeral_storage=\"20Gi\"),\n",
    "    requests=Resources(cpu=\"2\", mem=\"24Gi\", gpu=\"1\", ephemeral_storage=\"20Gi\"),\n",
    "    accelerator=L4,\n",
    "    min_replicas=0,\n",
    "    max_replicas=1,\n",
    "    scaledown_after=timedelta(minutes=10),\n",
    "    scaling_metric=ScalingMetric.Concurrency(2),\n",
    ")\n",
    "\n",
    "# union deploy apps 2_cv_images/app.py vlm-gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 2_cv_images/main.py\n",
    "\"\"\"\n",
    "This is a Gradio app for the SmolVLM-Instruct model.\n",
    "You can upload an image or take photo with webcam and get a vision-language response.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# Load model from Union artifact or fallback local path\n",
    "try:\n",
    "    from union_runtime import get_input\n",
    "    model_path = Path(get_input(\"downloaded-model\"))\n",
    "except:\n",
    "    model_path = Path(\"saved_model\")\n",
    "\n",
    "# Load processor and model\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_path, trust_remote_code=True).eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Inference function\n",
    "def vlm_infer(image: Image.Image) -> str:\n",
    "    start = time.time()\n",
    "\n",
    "    # Prompt with <image> placeholder required by IDEFICS3\n",
    "    prompt = \"<|user|>\\n<image>\\nWhatâ€™s going on in this photo?\\n<|end|>\\n<|assistant|>\"\n",
    "\n",
    "    # Pass all inputs as keyword arguments to avoid conflict\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "    # Decode output\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    latency = (time.time() - start) * 1000\n",
    "\n",
    "    return f\"{generated_text.strip()}\\n\\nâš¡ {device.type.upper()} | {latency:.1f} ms\"\n",
    "\n",
    "# Gradio UI\n",
    "demo = gr.Interface(\n",
    "    fn=vlm_infer,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload or Take a Photo\"),\n",
    "    outputs=gr.Text(label=\"SmolVLM Output\"),\n",
    "    title=\"SmolVLM-Instruct: Vision-Language Model\",\n",
    "    description=\"Upload an image to generate a vision-language response using SmolVLM (IDEFICS3).\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=8080)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6675ee1",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
